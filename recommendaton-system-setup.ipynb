{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "# take environment variables from .env.\n",
    "load_dotenv(override=True) \n",
    "\n",
    "search_service_endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "credential = AzureKeyCredential(os.getenv(\"AZURE_AI_SEARCH_API_KEY\", \"\")) if len(os.getenv(\"AZURE_AI_SEARCH_API_KEY\", \"\")) > 0 else DefaultAzureCredential()\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX\", \"recommendationidx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Search Index in Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "from dotenv import load_dotenv\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import (\n",
    "    VectorizedQuery,\n",
    ")\n",
    "from azure.search.documents.indexes.models import (\n",
    "    HnswAlgorithmConfiguration,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SimpleField,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchProfile,\n",
    "    SemanticSearch,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SemanticPrioritizedFields,\n",
    "    HnswParameters,\n",
    "    VectorSearchAlgorithmMetric\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "def create_or_update_index(client, index_name):\n",
    "    fields = [\n",
    "        SimpleField(\n",
    "            name=\"id\", \n",
    "            type=SearchFieldDataType.String, \n",
    "            key=True\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"title\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            searchable=True,\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"content\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            searchable=True,\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"embedding\",\n",
    "            # 8-bit signed integer (int8)\n",
    "            type=\"Collection(Edm.SByte)\",  \n",
    "            vector_search_dimensions=1024,\n",
    "            vector_search_profile_name=\"my-vector-config\",\n",
    "            # Use hidden=False if you want to return the embeddings in the search results\n",
    "            hidden=False, \n",
    "            searchable=True,\n",
    "            filterable=False,\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    semantic_search = SemanticSearch(\n",
    "                configurations=[\n",
    "                    SemanticConfiguration(\n",
    "                        name=\"my-semantic-config\",\n",
    "                        prioritized_fields=SemanticPrioritizedFields(\n",
    "                            title_field=SemanticField(field_name=\"title\"),\n",
    "                            content_fields=[\n",
    "                                SemanticField(field_name=\"content\"),\n",
    "                            ],\n",
    "                        ),\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    vector_search = VectorSearch(\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"my-vector-config\",\n",
    "                algorithm_configuration_name=\"my-hnsw\",\n",
    "            )\n",
    "        ],\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"my-hnsw\",\n",
    "                kind=VectorSearchAlgorithmKind.HNSW,\n",
    "                parameters=HnswParameters(\n",
    "                    metric=VectorSearchAlgorithmMetric.DOT_PRODUCT\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search, semantic_search=semantic_search)\n",
    "    return client.create_or_update_index(index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'recommendationidx' already exists.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Azure Search Index Client\n",
    "search_index_client = SearchIndexClient(\n",
    "    endpoint=search_service_endpoint,\n",
    "    credential=credential,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "# Check if the index exists\n",
    "try:\n",
    "    search_index_client.get_index(index_name)\n",
    "    print(f\"Index '{index_name}' already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Index '{index_name}' does not exist. Creating a new one.\")\n",
    "    create_or_update_index(search_index_client, index_name)\n",
    "    print(f\"Search index '{index_name}' created or updated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze and Clean data, if needed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Read all markdown files in the data directory\n",
    "md_files = glob.glob('data/*.md')\n",
    "\n",
    "# Read and display the first few rows of each markdown file\n",
    "for file in md_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    print(f\"Content of {file}:\")\n",
    "    # Display the first 100 characters of the file\n",
    "    print(content[:100]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunk documents to be indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 809 documents\n"
     ]
    }
   ],
   "source": [
    "# Chunk and load documents into AI search\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = DirectoryLoader('data/', glob=\"*.md\", loader_cls=TextLoader, loader_kwargs={'autodetect_encoding': True})\n",
    "docs = loader.load()\n",
    "titles = [doc.metadata['source'] for doc in docs]\n",
    "docs_with_titles = list(zip(titles, docs))\n",
    "documents = RecursiveCharacterTextSplitter(chunk_size=450, chunk_overlap=50).split_documents(docs)\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/product_info_2.md', 'data/product_info_10.md', 'data/customer_12.md', 'data/product_info_6.md', 'data/product_info_14.md', 'data/product_info_20.md', 'data/customer_4.md', 'data/product_info_7.md', 'data/product_info_15.md', 'data/customer_5.md', 'data/product_info_3.md', 'data/product_info_11.md', 'data/customer_13.md', 'data/customer_1.md', 'data/product_info_8.md', 'data/product_info_9.md', 'data/product_info_18.md', 'data/customer_8.md', 'data/product_info_19.md', 'data/customer_9.md', 'data/product_info_4.md', 'data/product_info_16.md', 'data/customer_6.md', 'data/product_info_12.md', 'data/customer_10.md', 'data/customer_2.md', 'data/product_info_1.md', 'data/product_info_13.md', 'data/customer_11.md', 'data/customer_3.md', 'data/product_info_5.md', 'data/product_info_17.md', 'data/customer_7.md']\n"
     ]
    }
   ],
   "source": [
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract page_content from each Document object\n",
    "document_texts = [doc.page_content for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Embeddings using Cohere embed V3\n",
    "- Use embed-english-v3.0 model to embed the data with 1024 Dimentions and 512 Context window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts, input_type=\"search_document\"):\n",
    "    model = \"embed-english-v3.0\"\n",
    "    # Ensure texts is a list\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    response = co.embed(\n",
    "        texts=texts,\n",
    "        model=model,\n",
    "        input_type=input_type,\n",
    "        embedding_types=[\"int8\"],\n",
    "    )\n",
    "    return [embedding for embedding in response.embeddings.int8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.ClientV2()\n",
    "\n",
    "# Limit to 80 calls per minute as Free Embed API has a limitation of 100 calls per min\n",
    "batch_size = 80\n",
    "embeddings = []\n",
    "for i in range(0, len(document_texts), batch_size):\n",
    "    batch = document_texts[i:i + batch_size]\n",
    "    embeddings.extend(generate_embeddings(batch))\n",
    "    if i + batch_size < len(document_texts):\n",
    "        time.sleep(60)  # Sleep for 60 seconds to respect the rate limit\n",
    "        \n",
    "print (len(embeddings), \"Document embeddings generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload documents and embeddings in Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_documents(search_client, documents, embeddings):\n",
    "    documents_to_index = [\n",
    "        {\"id\": str(idx), \"title\": titles[idx], \"content\": doc, \"embedding\": emb}\n",
    "        for idx, (doc, emb) in enumerate(zip(documents, embeddings))\n",
    "    ]\n",
    "    search_client.upload_documents(documents=documents_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SearchClient\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_service_endpoint, \n",
    "    index_name=index_name, \n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "# Index the documents and their embeddings\n",
    "index_documents(search_client, document_texts, embeddings)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
